start on 'abstractionless' UI
  port to JS now?

reconcile Kragen's primitives design
  It explains more of the action in interpreted code. This can matter
  for the user experience. Probably some combo of improvements to the
  compiler with hacks to the language design would reconcile that goal
  with the current (stupendous) performance. (But is this so important
  it should precede work on the UI?)

cmp ops need to deal with overriding
  What should `a == b` mean in general? or `a < b`?
  The current python code works sensibly only for primitive a and b.

finish parser
  support backslashes in '' and ""
  foo(name: x=y) [if actually wanted]

compiler:
(I question improving the compiler as the next goal, because 
nobody much cares about speedup of a language nobody uses.)
  don't call() known methods; invoke them directly.
    {me: foo=...me.foo}
    {foo=...}.foo
    0 < blah
  common subexpression elimination
    p.x * p.x
  add fast path for primitive ops
    x + y
  lift method definitions to highest possible scope
    ... {s=42} ...  
      #=> fortytwo = lambda _, me, k: k, 42  # (globally)
          ... {'s': fortytwo} ...
  fill cache eagerly sometimes?
  drop unused self-names
    {me: } => {}
  collapse extensions (does this ever come up?)
    a {s=...} {t=...}
  try a tracing jit approach

add error handling

add crude debugger or something?

Idea for speedup:
At parse time, notice patterns like  foo {...}.'()'
Represent it as a special expression type.
Implement it as invoking a special method on a BicicletaObject (bob).
The default method expands out to the usual steps.
Primitive bobs have specialized methods.

In a selfless slot thunk, check for a cached value on the ancestor?
The idea is that that won't change in computing the method on an
extension, since this method can't *refer* to the extension -- right?
I guess that could be violated by a mutual-recursive definition -- is
that possible?

Replace the trampoline with a state machine in one giant while loop.
Change the nested-tuple trampoline to a stack in an array.

Incorporate the miranda methods into the primitive method tables;
then call() on a primitive would never check more than one table.

Currently Bob inherits from dict where this self-dict is the slot
cache. If the self-dict were the methods table instead, then method
lookup might get slightly faster (we'd implement forwarding to the
parent using __missing__. Another refinement: different concrete Bob
classes for whether the parent is a primitive, another Bob, or
nothing, with a different __missing__ implementation for each; this
moves that test from lookup time to object-creation time.) Cost:
accessing the cache is self.cache[slot] instead of self[slot]. I
haven't measured any of this. It seemed obviously the way to go for
JS, and I'm just mentioning it here for Python for completeness.

Would it be faster with closures for continuations, instead of global
functions plus their arguments? I did things the latter way for the
sake of debugger access; but we could use callable objects that also
offer a debugger method.

trampoline: would trampolining only tail calls be enough? and faster?
(Is it even possible?) I'd guess it may be enough for
"programmer-like" use but not for the nice IDE.

You can make a recursive call using either something like env.fac or
like fac (the local self variable). Are there performance
implications? The latter way builds up a tower of overrides. To
address this, we could notice when we're shadowing the parent's full
method table, and inherit from grandparent instead. How to notice this
cheaply?


DEFERRED:

Make 1+1{} work somehow. (Needs a generic arithmetic scheme. I mean,
we could just make that case work, but generic arithmetic will affect
how you do it.)

In call, try testing if slot in self instead of if self.get(slot) is
not None; etc.
Result: imperceptible. And the code is a little nicer before than it'd
be this way for primitive method lookup. But come back to this after
incorporating miranda methods into the primitive method tabels.

Intern all the slot names? Seems to make no difference. (People on
e.g. stackoverflow say that interning keys in Python speeds up
dictionary lookup. Obviously it should if the dict knows that the key
you pass it was interned; but Python doesn't have a way like Common
Lisp to say this. It turns out the Python string struct includes a
field saying whether it's an intern result.) (I'd expect no speedup on
compiled code since Python string literals are interned already; but
any speed difference on the interpreter was lost in the noise.)


DONE:

Another idea (not really applicable in Python):
Method lookup should return two values: (ancestor, method).
Then you apply it as method(ancestor, self).
This would reduce consing in expressions like 2+3:
  2.'+'{arg1=3}.'()'
(But only if you don't have to cons the (ancestor, method) pair
onto the heap.)
This is because the final .'()' method would take the 2.'+' as the
ancestor argument and the extension holding arg1=3 as the self
argument.
In terp.py, OTOH, a new .'()' method is created each time (and the
method table holding it, too).
